{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89fdf93b",
   "metadata": {
    "height": 30,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q --upgrade langchain langchain-community langchain-chroma bs4 langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de829c7c",
   "metadata": {
    "height": 148,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "os.environ['USER_AGENT'] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f4f2e7e",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f58c258f",
   "metadata": {
    "height": 438,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "953d747d",
   "metadata": {
    "height": 319
   },
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1ebb93a",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d72f68dd",
   "metadata": {
    "height": 336
   },
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e604f70",
   "metadata": {
    "height": 353
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51f9f5e5",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run c6ef29e1-b6ba-4b3f-849a-c7e972501709 not found for run c9d1042c-0c26-4e2b-9898-a56459a43e3e. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a technique used to break down complex tasks into smaller and more manageable steps. This approach helps agents or models tackle difficult tasks by dividing them into simpler subtasks, making it easier to plan and execute. Task decomposition can be achieved through methods like Chain of Thought (CoT) or Tree of Thoughts, which guide the model in breaking down tasks effectively.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is Task Decomposition?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  \n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f290722e",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run fef22a86-4d6f-473a-aeef-288b5c9a9a86 not found for run 14237af4-9810-4240-8cc0-75bb131a8d3c. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Task decomposition can be done by (1) using Language Model (LLM) with simple prompting, such as asking for steps or subgoals, (2) providing task-specific instructions like \"Write a story outline,\" or (3) incorporating human inputs to guide the decomposition process effectively.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b495a3e",
   "metadata": {
    "height": 166
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User : What is Task Decomposition?\n",
      "\n",
      "AI : Task decomposition is a technique used to break down complex tasks into smaller and more manageable steps. This approach helps agents or models tackle difficult tasks by dividing them into simpler subtasks, making it easier to plan and execute. Task decomposition can be achieved through methods like Chain of Thought (CoT) or Tree of Thoughts, which guide the model in breaking down tasks effectively.\n",
      "\n",
      "User : What are common ways of doing it?\n",
      "\n",
      "AI : Task decomposition can be done in common ways such as using Language Model (LLM) with simple prompting, task-specific instructions, or human inputs. For example, LLM can be prompted with specific instructions like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" Alternatively, task-specific instructions like \"Write a story outline\" can guide the decomposition process for tasks such as writing a novel.\n",
      "\n",
      "User : What are common ways of doing it?\n",
      "\n",
      "AI : Task decomposition can be done by (1) using Language Model (LLM) with simple prompting, such as asking for steps or subgoals, (2) providing task-specific instructions like \"Write a story outline,\" or (3) incorporating human inputs to guide the decomposition process effectively.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "for message in store[\"abc123\"].messages : \n",
    "    if isinstance(message,AIMessage):\n",
    "        prefix = \"AI\"\n",
    "    else : \n",
    "        prefix = \"User\"\n",
    "    \n",
    "    print(f\"{prefix} : {message.content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1efb1e0c",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"blog_post_retriever\",\n",
    "    \"Searches and returns excerpts from the Autonomous Agents blog post.\"\n",
    ")\n",
    "tools = [tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7ea4089",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langgraph\n",
      "  Downloading langgraph-0.1.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.2 in /home/jovyan/.local/lib/python3.9/site-packages (from langgraph) (0.2.9)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.9/site-packages (from langchain-core<0.3,>=0.2->langgraph) (6.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/jovyan/.local/lib/python3.9/site-packages (from langchain-core<0.3,>=0.2->langgraph) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /home/jovyan/.local/lib/python3.9/site-packages (from langchain-core<0.3,>=0.2->langgraph) (0.1.82)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.9/site-packages (from langchain-core<0.3,>=0.2->langgraph) (24.0)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.9/site-packages (from langchain-core<0.3,>=0.2->langgraph) (1.10.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.9/site-packages (from langchain-core<0.3,>=0.2->langgraph) (8.3.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2->langgraph) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (3.10.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (2.28.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2->langgraph) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (3.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (2024.2.2)\n",
      "Downloading langgraph-0.1.1-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: langgraph\n",
      "Successfully installed langgraph-0.1.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a30f53ec",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent_executor = create_react_agent(llm,tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2169f36d",
   "metadata": {
    "height": 132
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='Task decomposition is a problem-solving strategy that involves breaking down a complex task or problem into smaller, more manageable sub-tasks. By decomposing a task, individuals can focus on addressing each sub-task separately, which can make the overall task easier to understand and complete successfully. This approach is commonly used in project management, software development, and various other fields to streamline the problem-solving process and improve efficiency.', response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 68, 'total_tokens': 149}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-7d0e68e4-b9e2-46e2-9b45-e4aea8e43585-0', usage_metadata={'input_tokens': 68, 'output_tokens': 81, 'total_tokens': 149})]}}\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "query = \"What is Task Decomposition?\"\n",
    "\n",
    "for s in agent_executor.stream({\"messages\" : [HumanMessage(content=query)]}) :\n",
    "    print(s)\n",
    "    print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "587209e5",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "agent_executor = create_react_agent(llm, tools, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9066e103",
   "metadata": {
    "height": 132
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='Hello Bob! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 67, 'total_tokens': 78}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4a8ba770-7ca6-4732-8cc3-2c4d4501a4af-0', usage_metadata={'input_tokens': 67, 'output_tokens': 11, 'total_tokens': 78})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "for s in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Hi! I'm bob\")]}, config=config\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99e4b819",
   "metadata": {
    "height": 132
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_D0koejiIm2dkVXKwbuMo4pQ3', 'function': {'arguments': '{\"query\":\"Task Decomposition\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 91, 'total_tokens': 110}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-cae0501a-3a93-43ea-aad4-90257148bf2d-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'Task Decomposition'}, 'id': 'call_D0koejiIm2dkVXKwbuMo4pQ3'}], usage_metadata={'input_tokens': 91, 'output_tokens': 19, 'total_tokens': 110})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:', name='blog_post_retriever', tool_call_id='call_D0koejiIm2dkVXKwbuMo4pQ3')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. This approach allows an agent or system to tackle challenging tasks by dividing them into more manageable components. By decomposing tasks, the overall complexity is reduced, making it easier to plan and execute the necessary actions to accomplish the task effectively.\\n\\nThere are different methods for task decomposition, such as Chain of Thought (CoT) and Tree of Thoughts, which involve breaking down tasks into sequential or branching steps. These methods help in guiding models to think step by step and explore multiple reasoning possibilities to achieve the desired outcome.\\n\\nTask decomposition can be carried out using language models with simple prompting, task-specific instructions, or human inputs. This process enables efficient planning and execution of tasks by organizing them into a structured hierarchy of subgoals and actions.\\n\\nIf you would like more information or details on this topic, feel free to ask!', response_metadata={'token_usage': {'completion_tokens': 179, 'prompt_tokens': 611, 'total_tokens': 790}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-7e006a4b-effc-4fab-a630-ae088761c436-0', usage_metadata={'input_tokens': 611, 'output_tokens': 179, 'total_tokens': 790})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Task Decomposition?\"\n",
    "\n",
    "for s in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]}, config=config\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a14e25ee",
   "metadata": {
    "height": 132
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_cIDg7XY1K3nFoBMhRbGy84fo', 'function': {'arguments': '{\"query\":\"common ways of task decomposition\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 813, 'total_tokens': 834}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-2f991338-7f49-4c0c-aa46-5542067112d5-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'common ways of task decomposition'}, 'id': 'call_cIDg7XY1K3nFoBMhRbGy84fo'}], usage_metadata={'input_tokens': 813, 'output_tokens': 21, 'total_tokens': 834})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.', name='blog_post_retriever', tool_call_id='call_cIDg7XY1K3nFoBMhRbGy84fo')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='According to the blog post, common ways of task decomposition include:\\n\\n1. Using Language Models (LLM) with Simple Prompting:\\n   - Language models can be utilized with simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" to break down tasks into smaller steps. This method helps in organizing complex tasks into more manageable components.\\n\\n2. Task-Specific Instructions:\\n   - Providing task-specific instructions such as \"Write a story outline\" for tasks like writing a novel. These instructions guide the decomposition of tasks based on the specific requirements of the task at hand.\\n\\n3. Human Inputs:\\n   - Task decomposition can also be done with human inputs, where individuals contribute their insights and perspectives to break down tasks effectively. Human inputs play a crucial role in dividing tasks into actionable steps.\\n\\nThese approaches to task decomposition aim to simplify complex tasks by breaking them down into smaller, more understandable components. By utilizing these methods, agents and systems can enhance their planning and execution processes to achieve successful outcomes.\\n\\nIf you have any more questions or need further clarification, feel free to ask!', response_metadata={'token_usage': {'completion_tokens': 221, 'prompt_tokens': 1358, 'total_tokens': 1579}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-9f19c66d-269c-4fdb-ac9e-6c0fd2fc0928-0', usage_metadata={'input_tokens': 1358, 'output_tokens': 221, 'total_tokens': 1579})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "query = \"What according to the blog post are common ways of doing it? redo the search\"\n",
    "\n",
    "for s in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]}, config=config\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
